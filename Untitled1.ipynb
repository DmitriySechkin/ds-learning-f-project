{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python\\conda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import FastText\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скачивание данных с api hh.ru для обучающей выборки\n",
    "Ограничением api hh.ru является то, что за один раз можно скачать только 2000 вакансий.\n",
    "Для получения большего количества вакансий были сформированы url с фильтрами так, что количесво записей получалось меньше 2000. Разбивка в основном была по регионам, профессиям, станциям метро. \n",
    "Скачать удалось не все, тк api выкидывает ошибки, если не использовать паузы. Для скачивания всей возможной выборки возможно потребуется использование нескольких экземпляров парсеров с разных ip адресов.\n",
    "Удалось скачать около 150 тысяч описаний вакансий. Резюме с api hh.ru получить не удалось. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ClusterId:\n",
    "    region: str = \"area\"\n",
    "    metro: str = \"metro\"\n",
    "    professional_role: str = \"professional_role\"\n",
    "    salary: str = \"salary\"\n",
    "    experience: str = \"experience\"\n",
    "\n",
    "class HhParser:\n",
    "\n",
    "    def __init__(self, cluster_id: ClusterId):\n",
    "        self._base_url = 'https://api.hh.ru/vacancies?clusters=true'\n",
    "        self.urls = []\n",
    "        self._cluster_id = cluster_id\n",
    "\n",
    "    def get_vacancy_cluster_urls(self):\n",
    "        json = self.get_json(self._base_url)\n",
    "        items = self.get_region_clusters(json)\n",
    "\n",
    "        self.urls.extend((i.get('url'), int(i.get('count'))) for i in self.filter_less_than_2000(items))\n",
    "\n",
    "        more_than_2000 = [i for i in self.filter_more_than_2000(items) if str(i.get('name')) != 'Россия']\n",
    "\n",
    "        extend_region_items = self.get_extend_region_clusters(more_than_2000)\n",
    "\n",
    "        self.urls.extend((i.get('url'), int(i.get('count'))) for i in self.filter_less_than_2000(extend_region_items))\n",
    "        more_than_2000 = [i for i in self.filter_more_than_2000(extend_region_items)]\n",
    "        more_than_2000.extend(i for i in extend_region_items if int(i.get('count')) == 0)\n",
    "\n",
    "        role_items = self.get_items_by_prof_role(more_than_2000)\n",
    "        self.urls.extend((i.get('url'), int(i.get('count'))) for i in self.filter_less_than_2000(role_items))\n",
    "        more_than_2000 = [i for i in self.filter_more_than_2000(role_items)]\n",
    "\n",
    "        experience_items = self.get_items_by_prof_role(more_than_2000)\n",
    "        self.urls.extend((i.get('url'), int(i.get('count'))) for i in self.filter_less_than_2000(experience_items))\n",
    "        more_than_2000 = [i for i in self.filter_more_than_2000(experience_items)]\n",
    "\n",
    "        self.urls = list(set(self.urls))\n",
    "\n",
    "        print(f'The total number of {len(self.urls)} URLs collected')\n",
    "\n",
    "\n",
    "    def get_extend_region_clusters(self, region_items):\n",
    "        items = []\n",
    "\n",
    "        for item in tqdm(region_items, 'Get extend region clusters'):\n",
    "            url = item.get('url')\n",
    "\n",
    "            if url is None:\n",
    "                continue\n",
    "\n",
    "            time.sleep(2)\n",
    "            json = self.get_json(url)\n",
    "\n",
    "            metro_items = self.get_metro_clusters(json)\n",
    "            metro_items = self.filter_metro_station_items(metro_items)\n",
    "\n",
    "            if len(metro_items) > 0:\n",
    "                items.extend(metro_items)\n",
    "            else:\n",
    "                extend_region_items = self.get_region_clusters(json)\n",
    "\n",
    "                if len(extend_region_items) > 1:\n",
    "                    items.extend(extend_region_items[:-1])\n",
    "                else:\n",
    "                    items.extend(extend_region_items)\n",
    "        return items\n",
    "\n",
    "    def get_items_by_prof_role(self, extend_items):\n",
    "        items = []\n",
    "\n",
    "        for item in tqdm(extend_items, 'Get items by prof role'):\n",
    "            url = item.get('url')\n",
    "\n",
    "            if url is None:\n",
    "                continue\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            json = self.get_json(url)\n",
    "            prof_role_items = self.get_role_clusters(json)\n",
    "\n",
    "            items.extend(prof_role_items)\n",
    "\n",
    "        return items\n",
    "\n",
    "    def get_items_by_experience(self, role_items):\n",
    "        items = []\n",
    "\n",
    "        for item in tqdm(role_items, 'Get items by experience'):\n",
    "            url = item.get('url')\n",
    "\n",
    "            if url is None:\n",
    "                continue\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            json = self.get_json(url)\n",
    "            experience_items = self.get_experience_clusters(json)\n",
    "\n",
    "            items.extend(experience_items)\n",
    "\n",
    "        return items\n",
    "\n",
    "    def filter_less_than_2000(self, items):\n",
    "        return [i for i in items if int(i.get('count')) <= 2000 and int(i.get('count')) != 0]\n",
    "\n",
    "    def filter_more_than_2000(self, items):\n",
    "        return [i for i in items if int(i.get('count')) > 2000]\n",
    "\n",
    "    def get_cluster_items(self, json, cluster_id):\n",
    "        clusters = json.get('clusters')\n",
    "\n",
    "        if not clusters:\n",
    "            print('key error: clusters')\n",
    "            return []\n",
    "\n",
    "        cluster = self.get_cluster_by_id(clusters, cluster_id)\n",
    "\n",
    "        if not cluster:\n",
    "            return []\n",
    "\n",
    "        items = cluster.get('items')\n",
    "\n",
    "        if not items:\n",
    "            print('key error: items')\n",
    "            return []\n",
    "\n",
    "        return items\n",
    "\n",
    "    def get_cluster_by_id(self, clusters, cluster_id):\n",
    "        for cluster in clusters:\n",
    "            _id = str(cluster.get('id'))\n",
    "\n",
    "            if _id == cluster_id:\n",
    "                return cluster\n",
    "\n",
    "        return []\n",
    "\n",
    "    def get_region_clusters(self, json_data):\n",
    "        if json_data:\n",
    "            items = self.get_cluster_items(json_data, self._cluster_id.region)\n",
    "            return  items\n",
    "        else:\n",
    "            print('get_region_clusters: Not found json in get response!')\n",
    "            return []\n",
    "\n",
    "    def get_metro_clusters(self, json_data):\n",
    "        if json_data:\n",
    "            items = self.get_cluster_items(json_data, self._cluster_id.metro)\n",
    "            return items\n",
    "        else:\n",
    "            print('get_metro_clusters: Not found json in get response!')\n",
    "            return []\n",
    "\n",
    "    def get_role_clusters(self, json_data):\n",
    "        if json_data:\n",
    "            items = self.get_cluster_items(json_data, self._cluster_id.professional_role)\n",
    "            return items\n",
    "        else:\n",
    "            print('get_role_clusters: Not found json in get response!')\n",
    "            return []\n",
    "\n",
    "    def get_experience_clusters(self, json_data):\n",
    "        if json_data:\n",
    "            items = self.get_cluster_items(json_data, self._cluster_id.experience)\n",
    "            return items\n",
    "        else:\n",
    "            print('get_role_clusters: Not found json in get response!')\n",
    "            return []\n",
    "\n",
    "    def filter_metro_station_items(self, metro_items):\n",
    "        return [i for i in metro_items if str(i.get('type')) == 'metro_station']\n",
    "\n",
    "    def get_json(self, url):\n",
    "        response = requests.get(url, verify=False)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            json = response.json()\n",
    "\n",
    "            if not json:\n",
    "                return None;\n",
    "\n",
    "            return json\n",
    "        else:\n",
    "            print(f\"Failed to get json data by url {url}, status {response.status_code}, text - {response.text}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = ClusterId()\n",
    "\n",
    "parser = HhParser(cluster_id)\n",
    "parser.get_vacancy_cluster_urls()\n",
    "\n",
    "sorted_list = sorted(parser.urls, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединяем url так, чтобы количество записей по ним было максимальным, но не больше 2000\n",
    "def parse_and_modify_urls(url_tuples):\n",
    "    url_params_map = {}\n",
    "\n",
    "    # Группируем URL по набору имен параметров\n",
    "    for url, count in url_tuples:\n",
    "        parsed_url = urlparse(url)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        query_params.pop('clusters', None)  # Убираем параметр 'clusters'\n",
    "        query_param_names = sorted(query_params.keys())  # Получаем наименования параметров и сортируем их\n",
    "        query_param_key = ','.join(query_param_names)\n",
    "\n",
    "        if query_param_key not in url_params_map:\n",
    "            url_params_map[query_param_key] = []\n",
    "\n",
    "        url_params_map[query_param_key].append((url, count))\n",
    "\n",
    "    modified_urls = []\n",
    "\n",
    "    # Формируем новые URL с суммой количества, не превышающей 2000\n",
    "    for query_param_key, url_count_pairs in url_params_map.items():\n",
    "        total_count = 0\n",
    "        combined_params = []\n",
    "\n",
    "        # Набираем параметры для нового URL\n",
    "        for url, count in url_count_pairs:\n",
    "\n",
    "            if total_count + count <= 2000:\n",
    "                total_count += count\n",
    "                parsed_url = urlparse(url)\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "                for param_name, param_values in query_params.items():\n",
    "                    combined_params.append((param_name, param_values[0]))  # Добавляем все значения параметров\n",
    "            else:\n",
    "\n",
    "                # Убираем параметр 'clusters' из объединенных параметров, если он есть\n",
    "                combined_params = [(param_name, param_value) for param_name, param_value in combined_params if param_name != 'clusters']\n",
    "\n",
    "                # Преобразуем объединенные параметры в строку\n",
    "                new_query = urlencode(combined_params, doseq=True)\n",
    "                new_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}\"\n",
    "                modified_urls.append((new_url, total_count))  # Добавляем новый URL с суммарным количеством\n",
    "\n",
    "                combined_params = []\n",
    "                total_count = count\n",
    "\n",
    "                parsed_url = urlparse(url)\n",
    "                query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "                for param_name, param_values in query_params.items():\n",
    "                    combined_params.append((param_name, param_values[0]))  # Добавляем все значения параметров\n",
    "\n",
    "        # Убираем параметр 'clusters' из объединенных параметров, если он есть\n",
    "        combined_params = [(param_name, param_value) for param_name, param_value in combined_params if param_name != 'clusters']\n",
    "\n",
    "        # Преобразуем объединенные параметры в строку\n",
    "        new_query = urlencode(combined_params, doseq=True)\n",
    "        new_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}\"\n",
    "        modified_urls.append((new_url, total_count))  # Добавляем новый URL с суммарным количеством\n",
    "\n",
    "    return modified_urls\n",
    "\n",
    "\n",
    "modified_urls = parse_and_modify_urls(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataSourcePdAdapter():\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, directory_path, file_name):\n",
    "        self._directory_path = directory_path\n",
    "        self._file_name = file_name\n",
    "        self._extension = ''        \n",
    "        self._check_filename()\n",
    "\n",
    "    def _get_full_path(self):\n",
    "        path = os.path.join(self._directory_path, f'{self._file_name}.{self._extension}')\n",
    "        return path\n",
    "\n",
    "    def _check_directory_path(self):\n",
    "        if self._directory_path == '':\n",
    "            return \n",
    "        if not os.path.exists(self._directory_path):\n",
    "            os.mkdir(self._directory_path)\n",
    "\n",
    "        if not os.path.exists(self._directory_path):\n",
    "            raise Exception(f'Ошибка при создании каталога {self._directory_path}')\n",
    "\n",
    "    def _check_filename(self):\n",
    "        if self._file_name == '':\n",
    "            raise Exception(f'file_name не может быть пустой строкой!')\n",
    "\n",
    "    @abstractmethod\n",
    "    def write(self, df):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _read(self):\n",
    "        pass\n",
    "\n",
    "class CsvPdAdapter(DataSourcePdAdapter):\n",
    "\n",
    "    def __init__(self, directory_path, file_name, sep=','):\n",
    "        super().__init__(directory_path, file_name)\n",
    "        self._sep = sep\n",
    "        self._extension = 'csv'\n",
    "\n",
    "    def write(self, df):\n",
    "        df.to_csv(self._get_full_path(), index=False, sep=self._sep)\n",
    "\n",
    "    def read(self, n_rows=None, chunksize=None):\n",
    "        path = self._get_full_path()\n",
    "        print(path)\n",
    "        return pd.read_csv(path, sep=self._sep, chunksize=chunksize, nrows=n_rows, engine='python')\n",
    "\n",
    "class SqlLitePdAdapter(DataSourcePdAdapter):\n",
    "    def __init__(self, directory_path, file_name, table_name):\n",
    "        super().__init__(directory_path, file_name)\n",
    "        self._table = table_name\n",
    "        self._operation_type = 'append'\n",
    "        self._extension = 'db'\n",
    "\n",
    "        self._create_table()\n",
    "\n",
    "    def _create_table(self):\n",
    "        with sqlite3.connect(self._get_full_path()) as con:\n",
    "            con.execute(f\"\"\"\n",
    "                CREATE TABLE if not exists {self._table} (\n",
    "                    id INTEGER PRIMARY KEY,\n",
    "                    name VARCHAR(100),\n",
    "                    description TEXT,\n",
    "                    branded_description TEXT,\n",
    "                    key_skills VARCHAR(200),\n",
    "                    professional_roles VARCHAR(200)\n",
    "\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "    def write(self, df):\n",
    "        self._check_directory_path()\n",
    "        with sqlite3.connect(self._get_full_path()) as con:\n",
    "            df.to_sql(self._table, con, index=False, if_exists=\"replace\")\n",
    "\n",
    "    def write_row(self, vacancy):\n",
    "        self._check_directory_path()\n",
    "        \n",
    "        with sqlite3.connect(self._get_full_path()) as connection:\n",
    "            cursor = connection.cursor()\n",
    "\n",
    "            try:\n",
    "                  cursor.execute(\n",
    "                      f\"\"\"\n",
    "                      INSERT INTO {self._table}\n",
    "                      (\n",
    "                        id, name, description, branded_description, key_skills, professional_roles\n",
    "                      )\n",
    "                      VALUES (?, ?, ?, ?, ?, ?)\n",
    "                      \"\"\", (vacancy.id, vacancy.name, vacancy.description, vacancy.branded_description,\n",
    "                            vacancy.key_skills, vacancy.professional_roles)\n",
    "                )\n",
    "            except Exception as ex:\n",
    "                print(f'vacancy - {str(vacancy)}')\n",
    "                print(f'ошибка при записи в БД - {ex}')\n",
    "                \n",
    "                raise ex\n",
    "\n",
    "    def read_all(self, chunksize=None):\n",
    "        query = f'select * from {self._table}'\n",
    "        with sqlite3.connect(self._get_full_path()) as con:   \n",
    "            con.text_factory = str\n",
    "            con.text_factory = lambda b: b.decode(errors='ignore')\n",
    "            return pd.read_sql(query, con, chunksize=chunksize)\n",
    "\n",
    "    def read(self, query_params_str='', target_columns=None):\n",
    "        if target_columns:\n",
    "            target_columns_srt = ', '.join(target_columns)\n",
    "        else:\n",
    "            target_columns_srt = '*'\n",
    "\n",
    "        query = f'select {target_columns_srt} from {self._table} {query_params_str}'\n",
    "        \n",
    "        with sqlite3.connect(self._get_full_path()) as con:\n",
    "            return pd.read_sql(query, con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite3_adapter = SqlLitePdAdapter('', 'vacancies', 'vacancies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Vacancy:\n",
    "    id: int\n",
    "    name: str\n",
    "    description: str\n",
    "    branded_description: str\n",
    "    key_skills: str\n",
    "    professional_roles: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение подробных данных по каждой вакансии, каждую запись сохраняем сразу в БД, ошибки игнорируем\n",
    "def fetch_vacancies(url):\n",
    "    per_page = 100\n",
    "    extracted_vacancies = 0\n",
    "    continues_vac = 0\n",
    "\n",
    "    ids = []\n",
    "\n",
    "    result_db = sqlite3_adapter.read(target_columns=['id'])   \n",
    "\n",
    "    total_pages = 2000 // per_page + (1 if 2000 % per_page != 0 else 0)\n",
    "    ids = []\n",
    "    \n",
    "    captcha_vacancies = []\n",
    "\n",
    "    for page in range(0, 1):  # получить данные по всем страницам не реально под одним ip\n",
    "        params = {'per_page': per_page, 'page': page}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "        except Exception as ex:\n",
    "            print(f'ошибка при получении списка вакансий {ex}')\n",
    "            continue\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            time.sleep(0.2)\n",
    "            vacancies = response.json()\n",
    "\n",
    "            for item in tqdm(vacancies['items']):\n",
    "                is_exists = result_db[\"id\"].isin([int(item['id'])]).any()\n",
    "\n",
    "                if is_exists or item['id'] in ids:\n",
    "                    continues_vac += 1\n",
    "                    continue\n",
    "\n",
    "                time.sleep(0.3)\n",
    "\n",
    "                try:\n",
    "                    result = get_vacancy(item['id'])\n",
    "                except Exception as ex:\n",
    "                    print(f'ошибка при получении вакансии - {ex}')\n",
    "                    continue\n",
    "\n",
    "                if 'captcha_required' in str(result):\n",
    "                    time.sleep(1)                    \n",
    "                    captcha_vacancies.append(item['id'])\n",
    "                    continue\n",
    "\n",
    "                if 'errors' in str(result):\n",
    "                    time.sleep(2)                    \n",
    "                    captcha_vacancies.append(item['id'])\n",
    "                    continue\n",
    "\n",
    "                _id = int(result.get('id'))\n",
    "                name = result.get('name')\n",
    "                description = result.get('description')\n",
    "                branded_description = result.get('branded_description')\n",
    "                key_skills = ' '.join(i.get('name') for i in result.get('key_skills'))\n",
    "                professional_roles = ' '.join(i.get('name') for i in result.get('professional_roles'))\n",
    "\n",
    "                vacancy = Vacancy(_id, name, description, branded_description, key_skills, professional_roles)\n",
    "\n",
    "                try:\n",
    "                    sqlite3_adapter.write_row(vacancy)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "                ids.append(_id)\n",
    "                extracted_vacancies += 1\n",
    "\n",
    "        elif response.status_code == 400 :\n",
    "            print(\"Reached the limit of 2000 items. Stopping further fetching.\")\n",
    "\n",
    "        else:\n",
    "            print(response.text)\n",
    "            print(f\"Error fetching vacancies. Status code: {response.status_code}\")\n",
    "\n",
    "    print(f'вакансий получено - {extracted_vacancies}')\n",
    "    print(f'вакансий пропущено - {continues_vac}')\n",
    "    print(f'вакансий с ошибкой - {len(captcha_vacancies)}')\n",
    "    \n",
    "    return captcha_vacancies\n",
    "\n",
    "def get_vacancy(id_vacancy):\n",
    "    target_url = f'https://api.hh.ru/vacancies/{id_vacancy}'\n",
    "    vacancy = requests.get(target_url).json()\n",
    "    return vacancy\n",
    "\n",
    "for ind in range(0, len(modified_urls) - 1):\n",
    "    url = modified_urls[ind]   \n",
    "    fetch_vacancies(url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame()\n",
    "chunk_size = 10000  \n",
    "\n",
    "for chunk in sqlite3_adapter.read_all(chunksize=chunk_size):\n",
    "    full_df = pd.concat([full_df, chunk], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149988"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединяем данные по вакансии в одно описание, нас интересуют название вакансии, ключевые навыки и описание вакансии.\n",
    "full_df['description_all'] = full_df['name'] + ' ' + full_df['description'] + ' ' + full_df['key_skills']\n",
    "sqlite3_adapter.write(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение тестовой выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестовая выборка на основе баз данных с сайта https://data.rcsi.science/data-catalog/datasets/186/#dataset-codebook \n",
    "Были получены описания вакансий и резюме, а так же отклики кандидатов на вакансии. По откликам столкнул вакансии и резюме.\n",
    "Для уменьшения размера выборки выбрал резюме, по которым было отправлено более 3-х откликов. \n",
    "По полученным id резюме выбрал описания опыта резюме, а так же id вакансий. \n",
    "Из полученной выборки выбрал 1 резюме и по названию должности искал 10 вакансий для резюме.\n",
    "Результат сохранил в csv resultResponsesTest. \n",
    "Так же в csv файле добавлены вручную id вакансий и некоторые тест-кейсы для получения более разнообразной выборки.\n",
    "Итоговая тестовая выборка собрана в файле resultResponsesTest\n",
    "\n",
    "В итоге собрано 325 резюме и покаждому по наименованию должности подобрано 10 описаний вакансий\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trud_vsem_path = r'D:\\vacancies_project'\n",
    "\n",
    "trud_response_csv_adapter = CsvPdAdapter(trud_vsem_path, 'responses', sep=';')\n",
    "trud_vacancies_csv_adapter = CsvPdAdapter(trud_vsem_path, 'vacancies', sep=';')\n",
    "trud_resume_csv_adapter = CsvPdAdapter(trud_vsem_path, 'workexp', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vacancies_project\\responses.csv\n"
     ]
    }
   ],
   "source": [
    "responses_df = pd.DataFrame()\n",
    "chunk_size = 10000  \n",
    "\n",
    "for chunk in trud_response_csv_adapter.read(chunksize=chunk_size):\n",
    "    responses_df = pd.concat([responses_df, chunk], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение выборки с откликами\n",
    "responses_df = responses_df[responses_df['response_type'] == 'Принятие']\n",
    "\n",
    "response_unique = responses_df.drop_duplicates(subset=['id_cv', 'id_vacancy'])\n",
    "\n",
    "group_count_df = response_unique\\\n",
    "                .groupby('id_cv', as_index=False)\\\n",
    "                .count()\\\n",
    "                .sort_values(by='activity_flag_candidate', ascending=False)\n",
    "\n",
    "filtered_response_df = group_count_df.query('activity_flag_candidate >= 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cv</th>\n",
       "      <th>activity_flag_candidate</th>\n",
       "      <th>activity_flag_manager</th>\n",
       "      <th>date_creation</th>\n",
       "      <th>date_creation_mistake</th>\n",
       "      <th>date_last_updated</th>\n",
       "      <th>date_modify</th>\n",
       "      <th>date_modify_mistake</th>\n",
       "      <th>id_candidate</th>\n",
       "      <th>id_hiring_organization</th>\n",
       "      <th>is_new</th>\n",
       "      <th>id_reply</th>\n",
       "      <th>id_response</th>\n",
       "      <th>id_vacancy</th>\n",
       "      <th>region_code</th>\n",
       "      <th>response_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120559</th>\n",
       "      <td>ca9afaf0-2478-11e8-ac03-037acc02728d</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>0</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>0</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>0771d920-73d8-11e5-b17c-239645b044d5</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id_cv  activity_flag_candidate  \\\n",
       "120559  ca9afaf0-2478-11e8-ac03-037acc02728d                      430   \n",
       "4331    0771d920-73d8-11e5-b17c-239645b044d5                       90   \n",
       "\n",
       "        activity_flag_manager  date_creation  date_creation_mistake  \\\n",
       "120559                    430            430                    430   \n",
       "4331                       90             90                     90   \n",
       "\n",
       "        date_last_updated  date_modify  date_modify_mistake  id_candidate  \\\n",
       "120559                430          430                    0           430   \n",
       "4331                   90           90                    0            90   \n",
       "\n",
       "        id_hiring_organization  is_new  id_reply  id_response  id_vacancy  \\\n",
       "120559                     430     430         0          430         430   \n",
       "4331                        90      90         0           90          90   \n",
       "\n",
       "        region_code  response_type  \n",
       "120559          430            430  \n",
       "4331             90             90  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_response_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cv</th>\n",
       "      <th>id_vacancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>4eaa0347-bc4c-11e6-94b6-0f468c90bfa7</td>\n",
       "      <td>9b1ac386-bbc6-11e6-9b24-736ab11edb0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7543</th>\n",
       "      <td>91c6a1c8-071d-11e8-91b8-ef76bd2a03c1</td>\n",
       "      <td>0cec9c46-778c-11e8-bb58-736ab11edb0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586279</th>\n",
       "      <td>06d34560-bf66-11ea-bd72-e37b4be0b9ed</td>\n",
       "      <td>ad13c4f6-7a6d-11eb-af9c-b905beff6f7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586280</th>\n",
       "      <td>0c103c08-9c40-11e7-b7a1-ef76bd2a03c1</td>\n",
       "      <td>d70e3206-3b00-11e7-adfe-037acc02728d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586285</th>\n",
       "      <td>28ce6a54-dc0d-11e5-9c7c-037acc02728d</td>\n",
       "      <td>fd9a99b6-3748-11e7-a8f8-4376a32b3f45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654591</th>\n",
       "      <td>faf1ae9a-f1e0-11e6-a868-736ab11edb0c</td>\n",
       "      <td>f0f7a2e6-07b7-11e7-8c0a-ef76bd2a03c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654592</th>\n",
       "      <td>91f292c0-119e-11ea-8d31-e37b4be0b9ed</td>\n",
       "      <td>68fd1ff2-9847-11e9-8ba1-bf2cfe8c828d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654594</th>\n",
       "      <td>0ad22ad8-ccc8-11e6-b9e5-ef76bd2a03c1</td>\n",
       "      <td>32b96f46-91f4-11e6-b83e-4376a32b3f45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654596</th>\n",
       "      <td>069687d0-3e25-11e7-a296-736ab11edb0c</td>\n",
       "      <td>e7651437-0741-11e8-8956-037acc02728d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654602</th>\n",
       "      <td>bdeb11d7-d4e8-11e7-bcaf-ef76bd2a03c1</td>\n",
       "      <td>5d6d40c6-fd9e-11e6-afee-736ab11edb0c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100141 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id_cv  \\\n",
       "7542     4eaa0347-bc4c-11e6-94b6-0f468c90bfa7   \n",
       "7543     91c6a1c8-071d-11e8-91b8-ef76bd2a03c1   \n",
       "586279   06d34560-bf66-11ea-bd72-e37b4be0b9ed   \n",
       "586280   0c103c08-9c40-11e7-b7a1-ef76bd2a03c1   \n",
       "586285   28ce6a54-dc0d-11e5-9c7c-037acc02728d   \n",
       "...                                       ...   \n",
       "1654591  faf1ae9a-f1e0-11e6-a868-736ab11edb0c   \n",
       "1654592  91f292c0-119e-11ea-8d31-e37b4be0b9ed   \n",
       "1654594  0ad22ad8-ccc8-11e6-b9e5-ef76bd2a03c1   \n",
       "1654596  069687d0-3e25-11e7-a296-736ab11edb0c   \n",
       "1654602  bdeb11d7-d4e8-11e7-bcaf-ef76bd2a03c1   \n",
       "\n",
       "                                   id_vacancy  \n",
       "7542     9b1ac386-bbc6-11e6-9b24-736ab11edb0c  \n",
       "7543     0cec9c46-778c-11e8-bb58-736ab11edb0c  \n",
       "586279   ad13c4f6-7a6d-11eb-af9c-b905beff6f7a  \n",
       "586280   d70e3206-3b00-11e7-adfe-037acc02728d  \n",
       "586285   fd9a99b6-3748-11e7-a8f8-4376a32b3f45  \n",
       "...                                       ...  \n",
       "1654591  f0f7a2e6-07b7-11e7-8c0a-ef76bd2a03c1  \n",
       "1654592  68fd1ff2-9847-11e9-8ba1-bf2cfe8c828d  \n",
       "1654594  32b96f46-91f4-11e6-b83e-4376a32b3f45  \n",
       "1654596  e7651437-0741-11e8-8956-037acc02728d  \n",
       "1654602  5d6d40c6-fd9e-11e6-afee-736ab11edb0c  \n",
       "\n",
       "[100141 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_cv_response = set(filtered_response_df['id_cv'])\n",
    "\n",
    "result_response_df = response_unique[response_unique['id_cv'].isin(id_cv_response)]\n",
    "result_response_df = result_response_df[['id_cv', 'id_vacancy']]\n",
    "result_response_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем id вакансий, которые будем читать из файла, тк файл огромный, все вакансии не нужны\n",
    "filtered_vacancies_response_df = responses_df[(responses_df['id_cv'].isin(id_cv_response)) & (responses_df['response_type'] == 'Принятие')]\n",
    "id_vacancies_response = set(filtered_vacancies_response_df['id_vacancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vacancies_project\\workexp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1217it [14:31,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# получаем резюме из csv \n",
    "result_resume_chunk_df = pd.DataFrame()\n",
    "chunk_size = 10000  \n",
    "\n",
    "for chunk in tqdm(trud_resume_csv_adapter.read(chunksize=chunk_size)):\n",
    "    filtered_chunk = chunk[chunk['id_cv'].isin(id_cv_response)]    \n",
    "    result_resume_chunk_df = pd.concat([result_resume_chunk_df, filtered_chunk], ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vacancies_project\\vacancies.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "443it [20:03,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# получаем вакансии из csv \n",
    "result_vacancies_chunk_df = pd.DataFrame()\n",
    "chunk_size = 30000  \n",
    "\n",
    "for chunk in tqdm(trud_vacancies_csv_adapter.read(chunksize=chunk_size)):\n",
    "    filtered_chunk = chunk[chunk['identifier'].isin(id_vacancies_response)]\n",
    "    result_vacancies_chunk_df = pd.concat([result_vacancies_chunk_df, filtered_chunk], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем датафрейм с описаниями вакансий\n",
    "result_vacancies_df = result_vacancies_chunk_df[['identifier', 'responsibilities', 'title', 'requirements_qualifications', 'employment_type', 'education_requirements_speciality', 'education_requirements_education_type']]\n",
    "result_vacancies_df.fillna('', inplace=True)\n",
    "result_vacancies_df['education_requirements_education_type'] = result_vacancies_df['education_requirements_education_type'].apply(lambda x: f'образование {x}' if x != '' else x)\n",
    "\n",
    "result_vacancies_df['vacancy_description'] = result_vacancies_df.apply(\n",
    "    lambda row: f\"{row['title']}\\\n",
    "    {row['responsibilities']}\\\n",
    "    {row['education_requirements_education_type']}\\\n",
    "    {row['education_requirements_speciality']}\\\n",
    "    {row['employment_type']}\", \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "professions = set(result_resume_chunk_df['job_title'])\n",
    "prof_vacancies = result_vacancies_df[result_vacancies_df['title'].isin(professions)].drop_duplicates('vacancy_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставляем только 10 вакансий на каждое название профессии\n",
    "filtered_prof_df = prof_vacancies.groupby('title').head(10)\n",
    "filtered_prof_df = filtered_prof_df.groupby('title').filter(lambda x: len(x) == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cv</th>\n",
       "      <th>resume_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38ea68a0-26e6-11e8-82b5-e37b4be0b9ed</td>\n",
       "      <td>Нач участка,мастер    2016-09-01        ООО \"Т...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38ea68a0-26e6-11e8-82b5-e37b4be0b9ed</td>\n",
       "      <td>Управляющий директор    2015-03-01    2016-08-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38ea68a0-26e6-11e8-82b5-e37b4be0b9ed</td>\n",
       "      <td>Начальник участка,мастер    2014-04-01    2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38ea68a0-26e6-11e8-82b5-e37b4be0b9ed</td>\n",
       "      <td>Начальник участка    2014-03-01    2015-09-01 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38ea68a0-26e6-11e8-82b5-e37b4be0b9ed</td>\n",
       "      <td>Начальник участка    2013-07-01    2014-03-01 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id_cv  \\\n",
       "0  38ea68a0-26e6-11e8-82b5-e37b4be0b9ed   \n",
       "1  38ea68a0-26e6-11e8-82b5-e37b4be0b9ed   \n",
       "2  38ea68a0-26e6-11e8-82b5-e37b4be0b9ed   \n",
       "3  38ea68a0-26e6-11e8-82b5-e37b4be0b9ed   \n",
       "4  38ea68a0-26e6-11e8-82b5-e37b4be0b9ed   \n",
       "\n",
       "                                  resume_description  \n",
       "0  Нач участка,мастер    2016-09-01        ООО \"Т...  \n",
       "1  Управляющий директор    2015-03-01    2016-08-...  \n",
       "2  Начальник участка,мастер    2014-04-01    2015...  \n",
       "3  Начальник участка    2014-03-01    2015-09-01 ...  \n",
       "4  Начальник участка    2013-07-01    2014-03-01 ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получение датафрейма с описаниями резюме\n",
    "result_resume_chunk_df.fillna('', inplace=True)\n",
    "result_resume_df_unique = result_resume_chunk_df.drop_duplicates()\n",
    "\n",
    "result_resume_df_unique['resume_description'] = result_resume_df_unique.apply(\n",
    "    lambda row: f\"{row['job_title']}\\\n",
    "    {row['date_from']}\\\n",
    "    {row['date_to']}\\\n",
    "    {row['company_name']}\\\n",
    "    {row['demands']}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "result_resume_df_unique = result_resume_df_unique[['id_cv', 'resume_description']]\n",
    "result_resume_df_unique = result_resume_df_unique.drop_duplicates()\n",
    "result_resume_df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# формирование тестовой выборки \n",
    "n = 0\n",
    "prof_df = pd.DataFrame()\n",
    "for title in tqdm(set(filtered_prof_df.title)):   \n",
    "    \n",
    "    for i in list(filtered_prof_df[filtered_prof_df.title == title].vacancy_description):\n",
    "        row = {}    \n",
    "        row['id_cv'] = n\n",
    "        row['resume_description'] = result_resume_df_unique[result_resume_df_unique['resume_description'].str.startswith(title)].resume_description.iloc[0]\n",
    "        row['id_vacancy'] = '' # добавил вручную в файле\n",
    "        row['vacancy_description'] = i\n",
    "        row_df = pd.DataFrame([row])\n",
    "        \n",
    "        prof_df = pd.concat([prof_df, row_df], ignore_index=True)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trud_result_csv_adapter = CsvPdAdapter(trud_vsem_path, 'resultResponsesTest', sep=';')\n",
    "trud_result_csv_adapter.write(prof_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vacancies_project\\resultResponsesTest.csv\n"
     ]
    }
   ],
   "source": [
    "test_vacancies_df = trud_result_csv_adapter.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение и обработка обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite3_adapter = SqlLitePdAdapter('', 'vacancies', 'vacancies')\n",
    "\n",
    "train_vacancies_df = pd.DataFrame()\n",
    "chunk_size = 10000  \n",
    "\n",
    "for chunk in sqlite3_adapter.read_all(chunksize=chunk_size):\n",
    "    train_vacancies_df = pd.concat([train_vacancies_df, chunk], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_vacancies_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>key_skills</th>\n",
       "      <th>professional_role_id</th>\n",
       "      <th>description_all</th>\n",
       "      <th>vacancy_description_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18330133</td>\n",
       "      <td>Агент по недвижимости (м. Пролетарская)</td>\n",
       "      <td>Компания ИНКОМ- Недвижимость уже 30 лет успешн...</td>\n",
       "      <td>Поиск и привлечение клиентов Активные продажи ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Агент по недвижимости (м. Пролетарская) Компан...</td>\n",
       "      <td>недвижимость лет успешно работает рынке мы при...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20155035</td>\n",
       "      <td>Руководитель отдела продаж (Private Banking)</td>\n",
       "      <td>АТОН - старейшая инвестиционная компания Росси...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>Руководитель отдела продаж (Private Banking) А...</td>\n",
       "      <td>руководитель отдела продаж private banking ато...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                          name  \\\n",
       "0  18330133       Агент по недвижимости (м. Пролетарская)   \n",
       "1  20155035  Руководитель отдела продаж (Private Banking)   \n",
       "\n",
       "                                         description  \\\n",
       "0  Компания ИНКОМ- Недвижимость уже 30 лет успешн...   \n",
       "1  АТОН - старейшая инвестиционная компания Росси...   \n",
       "\n",
       "                                          key_skills  professional_role_id  \\\n",
       "0  Поиск и привлечение клиентов Активные продажи ...                     1   \n",
       "1                                                                        2   \n",
       "\n",
       "                                     description_all  \\\n",
       "0  Агент по недвижимости (м. Пролетарская) Компан...   \n",
       "1  Руководитель отдела продаж (Private Banking) А...   \n",
       "\n",
       "                            vacancy_description_proc  \n",
       "0  недвижимость лет успешно работает рынке мы при...  \n",
       "1  руководитель отдела продаж private banking ато...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):   \n",
    "        import re\n",
    "        import spacy\n",
    "        \n",
    "        self.nlp = spacy.load(\"ru_core_news_sm\")\n",
    "        self.stop_words = self.nlp.Defaults.stop_words        \n",
    "\n",
    "    def spacy_tokenize(self, text):  \n",
    "        doc = self.nlp(text)  \n",
    "        \n",
    "        tokens = []\n",
    "        \n",
    "        for token in doc: \n",
    "            if token.is_space:\n",
    "                continue \n",
    "                \n",
    "            if token.lemma_ != '':\n",
    "                tokens.append(token.lemma_)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_text(self, text, is_stop_words=True):\n",
    "        import re\n",
    "        \n",
    "        words = []\n",
    "            \n",
    "        text = re.sub(r'(ООО|ПАО|ЗАО|ОАО|АО)([\\\"«])', r'\\1 \\2', text)\n",
    "        text = re.sub(r'[^А-ЯЁа-яёA-Za-z\\s]', '', text)    \n",
    "        \n",
    "        doc = self.nlp(text)       \n",
    "        ignored_tokens = set()\n",
    "        \n",
    "        for ent in doc.ents:            \n",
    "            if ent.label_ in [\"ORG\", \"LOC\"]:\n",
    "                ignored_tokens.update([token.text for token in ent])\n",
    "        \n",
    "        for word in doc:       \n",
    "            if word.is_punct or word.is_space:\n",
    "                continue\n",
    "            \n",
    "            if word.text in self.stop_words and is_stop_words:                \n",
    "                continue\n",
    "         \n",
    "            if word.text in ignored_tokens: \n",
    "                continue \n",
    "                \n",
    "            words.append(word.text.lower())\n",
    "        \n",
    "        text = ' '.join(words)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def clean_html_text(self, text):        \n",
    "        if not text:\n",
    "            return ''\n",
    "        \n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        \n",
    "        for script in soup([\"script\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        for style in soup.find_all('style'):\n",
    "            style.extract()\n",
    "\n",
    "        clean_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        return clean_text\n",
    "    \n",
    "processor = TextProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb87cebb3c04a54beeb5f8c3db17e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=37497), Label(value='0 / 37497')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# предобработка описаний вакансий в обучающей выборке\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "train_data['vacancy_description_proc'] = train_data['description_all'].parallel_apply(processor.preprocess_text)\n",
    "    \n",
    "sqlite3_adapter.write(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878c7a3d2ccd48d98ae4abd361d225c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=37497), Label(value='0 / 37497')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "train_data['vacancy_description_toc'] = train_data['vacancy_description_proc'].parallel_apply(processor.spacy_tokenize)\n",
    "    \n",
    "sqlite3_adapter.write(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer():\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, processor):\n",
    "        self._processor = processor\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, docs):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def vectorize(self, docs):\n",
    "        pass\n",
    "    \n",
    "    def process(self, docs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TFIDFVectorizer(Vectorizer):\n",
    "    def __init__(self, processor):\n",
    "        super().__init__(processor)     \n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit(self, docs):\n",
    "        start = time.time()\n",
    "        flat_docs = self.process(docs)\n",
    "        self.vectorizer.fit_transform(flat_docs)\n",
    "        end = time.time() - start\n",
    "        return end\n",
    "\n",
    "    def vectorize(self, docs):\n",
    "        flat_docs = self.process(docs)\n",
    "        return self.vectorizer.transform(flat_docs)\n",
    "\n",
    "    def process(self, docs):        \n",
    "        return [' '.join(self._processor.spacy_tokenize(doc)) for doc in docs]\n",
    "         \n",
    "\n",
    "class Word2VecVectorizer(Vectorizer):\n",
    "    def __init__(self, processor, size=2, window=5, min_count=1, workers=2):\n",
    "        super().__init__(processor)\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, docs):\n",
    "        start = time.time()\n",
    "        self.model = Word2Vec(docs, vector_size=self.size, min_count=self.min_count)\n",
    "        end = time.time() - start\n",
    "        return end\n",
    "\n",
    "    def vectorize(self, docs):\n",
    "        vectors = []\n",
    "        for words in docs:\n",
    "            words_vecs = [self.model.wv[word] for word in words if word in self.model.wv]\n",
    "    \n",
    "            if len(words_vecs) == 0:\n",
    "                words_vecs = np.zeros(self.size)       \n",
    "            \n",
    "            vectors.append(np.array(words_vecs).sum(axis=0))\n",
    "            \n",
    "        return np.array(vectors)\n",
    "    \n",
    "        \n",
    "class FastTextVectorizer(Vectorizer):\n",
    "    def __init__(self, processor, size=100, window=5, min_count=1, workers=2):\n",
    "        super().__init__(processor)\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, docs):\n",
    "        start = time.time()\n",
    "        self.model = FastText(\n",
    "              docs, \n",
    "              vector_size=self.size, \n",
    "              window=self.window, \n",
    "              min_count=self.min_count, \n",
    "              workers=self.workers\n",
    "        )\n",
    "        end = time.time() - start\n",
    "        return end\n",
    "        \n",
    "    def vectorize(self, docs):        \n",
    "        return np.array([np.mean([self.model.wv[word] for word in words if word in self.model.wv]\n",
    "                                 or [np.zeros(self.size)], axis=0)\n",
    "                         for words in docs])\n",
    "\n",
    "\n",
    "\n",
    "class BERTVectorizer:\n",
    "    def __init__(self, model_name='bert-base-multilingual-cased', batch_size=32):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def transform(self, texts):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = []\n",
    "            for i in range(0, len(texts), self.batch_size):\n",
    "                batch = texts[i:i+self.batch_size]\n",
    "                encoded_input = self.tokenizer(batch, return_tensors='pt', padding=True, truncation=True).to(self.device)\n",
    "                outputs = self.model(**encoded_input)\n",
    "                embeddings.append(outputs[1].cpu().numpy())\n",
    "            return np.vstack(embeddings)\n",
    "\n",
    "# Использование класса\n",
    "# bert_vectorizer = BERTVectorizer(batch_size=16)  # Можете настроить размер батча в зависимости от доступной памяти GPU\n",
    "# vectors = bert_vectorizer.transform([\"Пример текста\", \"Еще один текст\"])\n",
    "\n",
    "    \n",
    "# class BERTVectorizer(Vectorizer):\n",
    "#     def __init__(self, processor, model_name='bert-base-multilingual-cased'):\n",
    "#         super().__init__(processor)\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#         self.model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "#     def vectorize(self, docs):\n",
    "#         with torch.no_grad():\n",
    "#             return np.array(\n",
    "#                 [self.model(**self.tokenizer(text, return_tensors='pt', padding=True, truncation=True))[1].numpy() for text in docs]\n",
    "#             )\n",
    "    \n",
    "#     def process(self, docs):\n",
    "#         # нужен список строк, без лемматизации и токенизации, удаление стоп-слов и цифр\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vacancies_project\\resultResponsesTestProcessed2.csv\n"
     ]
    }
   ],
   "source": [
    "trud_vsem_path = r'D:\\vacancies_project'\n",
    "trud_result_csv_adapter = CsvPdAdapter(trud_vsem_path, 'resultResponsesTestProcessed2', sep=';')\n",
    "\n",
    "test_vacancies_df = trud_result_csv_adapter.read()\n",
    "data = test_vacancies_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 29s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['resume_description_proc'] = data['resume_description'].apply(processor.preprocess_text)\n",
    "data['vacancy_description_proc'] = data['vacancy_description'].apply(processor.preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cv</th>\n",
       "      <th>resume_description</th>\n",
       "      <th>id_vacancy</th>\n",
       "      <th>vacancy_description</th>\n",
       "      <th>resume_description_proc</th>\n",
       "      <th>vacancy_description_proc</th>\n",
       "      <th>resume_description_token</th>\n",
       "      <th>vacancy_description_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0023b346-73b3-11e8-a184-9122a281f90e</td>\n",
       "      <td>Водитель погрузчика    2007-11-01    2008-10-0...</td>\n",
       "      <td>1</td>\n",
       "      <td>водитель погрузчика    производить погрузку на...</td>\n",
       "      <td>водитель погрузчика             готовой продук...</td>\n",
       "      <td>водитель погрузчика     производить погрузку о...</td>\n",
       "      <td>['водитель', 'погрузчик', '            ', 'гот...</td>\n",
       "      <td>['водитель', 'погрузчик', '    ', 'производить...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0023b346-73b3-11e8-a184-9122a281f90e</td>\n",
       "      <td>Водитель погрузчика    2007-11-01    2008-10-0...</td>\n",
       "      <td>2</td>\n",
       "      <td>водитель погрузчика СРОЧНО!    производить пог...</td>\n",
       "      <td>водитель погрузчика             готовой продук...</td>\n",
       "      <td>водитель погрузчика     производить погрузку о...</td>\n",
       "      <td>['водитель', 'погрузчик', '            ', 'гот...</td>\n",
       "      <td>['водитель', 'погрузчик', '    ', 'производить...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id_cv  \\\n",
       "0  0023b346-73b3-11e8-a184-9122a281f90e   \n",
       "1  0023b346-73b3-11e8-a184-9122a281f90e   \n",
       "\n",
       "                                  resume_description  id_vacancy  \\\n",
       "0  Водитель погрузчика    2007-11-01    2008-10-0...           1   \n",
       "1  Водитель погрузчика    2007-11-01    2008-10-0...           2   \n",
       "\n",
       "                                 vacancy_description  \\\n",
       "0  водитель погрузчика    производить погрузку на...   \n",
       "1  водитель погрузчика СРОЧНО!    производить пог...   \n",
       "\n",
       "                             resume_description_proc  \\\n",
       "0  водитель погрузчика             готовой продук...   \n",
       "1  водитель погрузчика             готовой продук...   \n",
       "\n",
       "                            vacancy_description_proc  \\\n",
       "0  водитель погрузчика     производить погрузку о...   \n",
       "1  водитель погрузчика     производить погрузку о...   \n",
       "\n",
       "                            resume_description_token  \\\n",
       "0  ['водитель', 'погрузчик', '            ', 'гот...   \n",
       "1  ['водитель', 'погрузчик', '            ', 'гот...   \n",
       "\n",
       "                           vacancy_description_token  \n",
       "0  ['водитель', 'погрузчик', '    ', 'производить...  \n",
       "1  ['водитель', 'погрузчик', '    ', 'производить...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:58: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:58: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "C:\\Users\\Дмитрий\\AppData\\Local\\Temp\\ipykernel_7480\\191955338.py:58: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(cos_sim) == len(set(data['id_cv'].drop_duplicates())), f'cos_sim {len(cos_sim)} id_cv {0}')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "# Перечисляем методы векторизации\n",
    "# Функция для вычисления метрик\n",
    "def calculate_metrics(predicted, actual):\n",
    "    predicted = set(predicted)\n",
    "    actual = set(actual)\n",
    "    \n",
    "    tp = len(predicted & actual)\n",
    "    fp = len(predicted - actual)\n",
    "    fn = len(actual - predicted)\n",
    "    \n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def compute_test(name, vectorizer, results, data):\n",
    "    print(name)\n",
    "    start = time.time()\n",
    "    data_resume = data.drop_duplicates(['id_cv', 'resume_description_token'])\n",
    "    \n",
    "    if name == 'BERT':\n",
    "        resume_vecs = vectorizer.vectorize(data_resume['resume_description_proc'])\n",
    "        vacancy_vecs = vectorizer.vectorize(data['vacancy_description_proc'])\n",
    "    else:        \n",
    "        resume_vecs = vectorizer.vectorize(data_resume['resume_description_token'])\n",
    "        vacancy_vecs = vectorizer.vectorize(data['vacancy_description_token'])\n",
    "    \n",
    "    # Вычисление косинусного сходства и предсказание\n",
    "    cos_sim = cosine_similarity(resume_vecs, vacancy_vecs)  \n",
    "    print(len(cos_sim))\n",
    "    \n",
    "    top_10_preds = np.argsort(-cos_sim, axis=1)[:, :10]\n",
    "    print(len(top_10_preds))\n",
    "    \n",
    "    end_time =  time.time() - start\n",
    "    \n",
    "    assert(len(cos_sim) == len(set(data_resume['id_cv'])), f'cos_sim {len(cos_sim)} id_cv {0}')\n",
    "    \n",
    "    # Сохраняем результаты\n",
    "    precision_scores, recall_scores, f1_scores = [], [], []\n",
    "    \n",
    "    for idx, row in enumerate(data_resume['id_cv']):        \n",
    "        actual_ids = data[data['id_cv'] == row]['id_vacancy'].tolist()\n",
    "        \n",
    "        predicted_indices = top_10_preds[idx]\n",
    "    \n",
    "        # Конвертируем предсказанные индексы вакансий в фактические идентификаторы\n",
    "        predicted_ids = [data.iloc[i]['id_vacancy'] for i in predicted_indices]\n",
    "        \n",
    "#         print(predicted_ids)\n",
    "        \n",
    "#         for id_ in predicted_ids:\n",
    "#             print(data[data['id_vacancy'] == id_].vacancy_description_token.iloc[0])\n",
    "        \n",
    "        prec, rec, f1 = calculate_metrics(predicted_ids, actual_ids)\n",
    "        precision_scores.append(prec)\n",
    "        recall_scores.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "    results[name] = {\n",
    "        'Precision': np.mean(precision_scores),\n",
    "        'Recall': np.mean(recall_scores),\n",
    "        'F1 Score': np.mean(f1_scores),\n",
    "        'time': end_time,\n",
    "        'cos_sim': np.mean(top_10_preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cv</th>\n",
       "      <th>resume_description</th>\n",
       "      <th>id_vacancy</th>\n",
       "      <th>vacancy_description</th>\n",
       "      <th>resume_description_proc</th>\n",
       "      <th>vacancy_description_proc</th>\n",
       "      <th>resume_description_token</th>\n",
       "      <th>vacancy_description_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0023b346-73b3-11e8-a184-9122a281f90e</td>\n",
       "      <td>Водитель погрузчика    2007-11-01    2008-10-0...</td>\n",
       "      <td>1</td>\n",
       "      <td>водитель погрузчика    производить погрузку на...</td>\n",
       "      <td>водитель погрузчика             готовой продук...</td>\n",
       "      <td>водитель погрузчика     производить погрузку о...</td>\n",
       "      <td>['водитель', 'погрузчик', '            ', 'гот...</td>\n",
       "      <td>['водитель', 'погрузчик', '    ', 'производить...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0023b346-73b3-11e8-a184-9122a281f90e</td>\n",
       "      <td>Водитель погрузчика    2007-11-01    2008-10-0...</td>\n",
       "      <td>2</td>\n",
       "      <td>водитель погрузчика СРОЧНО!    производить пог...</td>\n",
       "      <td>водитель погрузчика             готовой продук...</td>\n",
       "      <td>водитель погрузчика     производить погрузку о...</td>\n",
       "      <td>['водитель', 'погрузчик', '            ', 'гот...</td>\n",
       "      <td>['водитель', 'погрузчик', '    ', 'производить...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id_cv  \\\n",
       "0  0023b346-73b3-11e8-a184-9122a281f90e   \n",
       "1  0023b346-73b3-11e8-a184-9122a281f90e   \n",
       "\n",
       "                                  resume_description  id_vacancy  \\\n",
       "0  Водитель погрузчика    2007-11-01    2008-10-0...           1   \n",
       "1  Водитель погрузчика    2007-11-01    2008-10-0...           2   \n",
       "\n",
       "                                 vacancy_description  \\\n",
       "0  водитель погрузчика    производить погрузку на...   \n",
       "1  водитель погрузчика СРОЧНО!    производить пог...   \n",
       "\n",
       "                             resume_description_proc  \\\n",
       "0  водитель погрузчика             готовой продук...   \n",
       "1  водитель погрузчика             готовой продук...   \n",
       "\n",
       "                            vacancy_description_proc  \\\n",
       "0  водитель погрузчика     производить погрузку о...   \n",
       "1  водитель погрузчика     производить погрузку о...   \n",
       "\n",
       "                            resume_description_token  \\\n",
       "0  ['водитель', 'погрузчик', '            ', 'гот...   \n",
       "1  ['водитель', 'погрузчик', '            ', 'гот...   \n",
       "\n",
       "                           vacancy_description_token  \n",
       "0  ['водитель', 'погрузчик', '    ', 'производить...  \n",
       "1  ['водитель', 'погрузчик', '    ', 'производить...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Определяем функцию, которая будет вызываться параллельно для каждого документа\n",
    "# def tokenize_document(doc):\n",
    "#     return processor.spacy_tokenize(doc)\n",
    "\n",
    "# # Распараллеливаем выполнение функции для каждого документа\n",
    "# train_tokens_parallel = Parallel(n_jobs=-1)(delayed(tokenize_document)(doc) for doc in tqdm(train_data['vacancy_description_proc'].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fasttext_vectorizer = FastTextVectorizer(processor)\n",
    "tfidf_vectorizer = TFIDFVectorizer(processor)\n",
    "word2vec_vectorizer = Word2Vec(processor)\n",
    "bert_vectorizer = BERTVectorizer()\n",
    "\n",
    "total_data = test_data['vacancy_description_token'].tolist()\\\n",
    "                    + train_data['vacancy_description_toc'].tolist()\\\n",
    "                    + test_data['resume_description_token'].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learning_time_tfidf = tfidf_vectorizer.fit(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learning_time_fasttext = fasttext_vectorizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learning_time_word2vec = word2vec_vectorizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Создаем словарь для результатов\n",
    "results = {}\n",
    "\n",
    "vectorizers = {\n",
    "    'tfidf': tfidf_vectorizer,\n",
    "    'fastText': fasttext_vectorizer,\n",
    "    'word2vec': word2vec_vectorizer\n",
    "}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    compute_test(name, vectorizer, results, test_data)\n",
    "\n",
    "# Вывод результатов\n",
    "for method, metrics in results.items():\n",
    "    print(f\"Results for {method}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "tfidf_vectorizer = vectorizers['TFIDF']\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vectorizer = vectorizers['Word2Vec']\n",
    "word2vec_vectorizer.model.save('word2vec_model')\n",
    "\n",
    "fasttext_vectorizer = vectorizers['FastText']\n",
    "fasttext_vectorizer.model.save('fasttext_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
